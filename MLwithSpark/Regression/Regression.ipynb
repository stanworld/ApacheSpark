{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/stan/ApacheSpark/MLwithSpark/Regression/hour_noheader.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_data=raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records = raw_data.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first = records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1', u'2011-01-01', u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']\n"
     ]
    }
   ],
   "source": [
    "print first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17379\n"
     ]
    }
   ],
   "source": [
    "print num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mapping(rdd,idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of first categorical feature column: {u'1': 0, u'3': 1, u'2': 2, u'4': 3}\n"
     ]
    }
   ],
   "source": [
    "print \"Mapping of first categorical feature column: %s\" %get_mapping(records,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mappings = [get_mapping(records,i) for i in range(2,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_len = sum(map(len,mappings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_len = len(records.first()[11:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.2879', u'0.81', u'0', u'3']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.first()[11:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_len = num_len+cat_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length for categorical features: 57\n"
     ]
    }
   ],
   "source": [
    "print \"Feature vector length for categorical features: %s\" %cat_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(record):\n",
    "    cat_vec = np.zeros(cat_len)\n",
    "    i = 0\n",
    "    step = 0\n",
    "    for field in record[2:9]:\n",
    "        m = mappings[i]\n",
    "        idx = m[field]\n",
    "        cat_vec[idx+step] =1\n",
    "        i = i+1\n",
    "        step =step + len(m)\n",
    "    num_vec = np.array([float(field) for field in record[10:14]])\n",
    "    return np.concatenate((cat_vec,num_vec))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_label(record):\n",
    "    return float(record[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = records.map(lambda r: LabeledPoint(extract_label(r),extract_features(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_point = data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data:[u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']\n"
     ]
    }
   ],
   "source": [
    "print \"Raw data:\" + str(first[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:16.0\n"
     ]
    }
   ],
   "source": [
    "print \"Label:\" + str(first_point.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model feature vector: \n",
      "[1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.24,0.2879,0.81,0.0]\n"
     ]
    }
   ],
   "source": [
    "print \"Linear Model feature vector: \\n\" + str(first_point.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_dt(record):\n",
    "    return np.array(map(float,record[2:14]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dt = records.map(lambda r: LabeledPoint(extract_label(r),extract_features_dt(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_point_dt = data_dt.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1.0,0.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(first_point_dt.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method train in module pyspark.mllib.regression:\n",
      "\n",
      "train(cls, data, iterations=100, step=1.0, miniBatchFraction=1.0, initialWeights=None, regParam=0.0, regType=None, intercept=False, validateData=True) method of __builtin__.type instance\n",
      "    Train a linear regression model using Stochastic Gradient\n",
      "    Descent (SGD).\n",
      "    This solves the least squares regression formulation\n",
      "    \n",
      "        f(weights) = 1/(2n) ||A weights - y||^2,\n",
      "    \n",
      "    which is the mean squared error.\n",
      "    Here the data matrix has n rows, and the input RDD holds the\n",
      "    set of rows of A, each with its corresponding right hand side\n",
      "    label y. See also the documentation for the precise formulation.\n",
      "    \n",
      "    :param data:              The training data, an RDD of\n",
      "                              LabeledPoint.\n",
      "    :param iterations:        The number of iterations\n",
      "                              (default: 100).\n",
      "    :param step:              The step parameter used in SGD\n",
      "                              (default: 1.0).\n",
      "    :param miniBatchFraction: Fraction of data to be used for each\n",
      "                              SGD iteration (default: 1.0).\n",
      "    :param initialWeights:    The initial weights (default: None).\n",
      "    :param regParam:          The regularizer parameter\n",
      "                              (default: 0.0).\n",
      "    :param regType:           The type of regularizer used for\n",
      "                              training our model.\n",
      "    \n",
      "                              :Allowed values:\n",
      "                                 - \"l1\" for using L1 regularization (lasso),\n",
      "                                 - \"l2\" for using L2 regularization (ridge),\n",
      "                                 - None for no regularization\n",
      "    \n",
      "                                 (default: None)\n",
      "    \n",
      "    :param intercept:         Boolean parameter which indicates the\n",
      "                              use or not of the augmented representation\n",
      "                              for training data (i.e. whether bias\n",
      "                              features are activated or not,\n",
      "                              default: False).\n",
      "    :param validateData:      Boolean parameter which indicates if\n",
      "                              the algorithm should validate data\n",
      "                              before training. (default: True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LinearRegressionWithSGD.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method trainRegressor in module pyspark.mllib.tree:\n",
      "\n",
      "trainRegressor(cls, data, categoricalFeaturesInfo, impurity='variance', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0) method of __builtin__.type instance\n",
      "    Train a DecisionTreeModel for regression.\n",
      "    \n",
      "    :param data: Training data: RDD of LabeledPoint.\n",
      "                 Labels are real numbers.\n",
      "    :param categoricalFeaturesInfo: Map from categorical feature\n",
      "             index to number of categories.\n",
      "             Any feature not in this map is treated as continuous.\n",
      "    :param impurity: Supported values: \"variance\"\n",
      "    :param maxDepth: Max depth of tree.\n",
      "             E.g., depth 0 means 1 leaf node.\n",
      "             Depth 1 means 1 internal node + 2 leaf nodes.\n",
      "    :param maxBins: Number of bins used for finding splits at each\n",
      "             node.\n",
      "    :param minInstancesPerNode: Min number of instances required at\n",
      "             child nodes to create the parent split\n",
      "    :param minInfoGain: Min info gain required to create a split\n",
      "    :return: DecisionTreeModel\n",
      "    \n",
      "    Example usage:\n",
      "    \n",
      "    >>> from pyspark.mllib.regression import LabeledPoint\n",
      "    >>> from pyspark.mllib.tree import DecisionTree\n",
      "    >>> from pyspark.mllib.linalg import SparseVector\n",
      "    >>>\n",
      "    >>> sparse_data = [\n",
      "    ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n",
      "    ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),\n",
      "    ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n",
      "    ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))\n",
      "    ... ]\n",
      "    >>>\n",
      "    >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})\n",
      "    >>> model.predict(SparseVector(2, {1: 1.0}))\n",
      "    1.0\n",
      "    >>> model.predict(SparseVector(2, {1: 0.0}))\n",
      "    0.0\n",
      "    >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])\n",
      "    >>> model.predict(rdd).collect()\n",
      "    [1.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTree.trainRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_model = LinearRegressionWithSGD.train(data,iterations=10,step=0.1,intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_vs_predicted = data.map(lambda p: (p.label,linear_model.predict(p.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model predictions: [(16.0, 117.89250386724845), (40.0, 116.2249612319211), (32.0, 116.02369145779235), (13.0, 115.67088016754433), (1.0, 115.56315650834316)]\n"
     ]
    }
   ],
   "source": [
    "print \"Linear Model predictions: \"+ str(true_vs_predicted.take(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_model = DecisionTree.trainRegressor(data_dt,{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = dt_model.predict(data_dt.map(lambda p: p.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actual = data.map(lambda p: p.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_vs_predicted_dt = actual.zip(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree predictions: [(16.0, 54.913223140495866), (40.0, 54.913223140495866), (32.0, 53.171052631578945), (13.0, 14.284023668639053), (1.0, 14.284023668639053)]\n"
     ]
    }
   ],
   "source": [
    "print \"Decision Tree predictions: \" + str(true_vs_predicted_dt.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree depth: 5\n"
     ]
    }
   ],
   "source": [
    "print \"Decision Tree depth: \"+ str(dt_model.depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree number of nodes: 63\n"
     ]
    }
   ],
   "source": [
    "print \"Decision Tree number of nodes: \"+ str(dt_model.numNodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the performance of regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error(actual, pred):\n",
    "    return (pred-actual)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abs_error(actual, pred):\n",
    "    return np.abs(pred-actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_log_error(pred,actual):\n",
    "    return (np.log(pred+1)-np.log(actual+1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse = true_vs_predicted.map(lambda(t,p):squared_error(t,p)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae = true_vs_predicted.map(lambda(t,p):abs_error(t,p)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsle = np.sqrt(true_vs_predicted.map(lambda (t,p):squared_log_error(t,p)).mean()) ## root mean squared log error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model- Mean squared error, mean absolute error, root mean squared log error: 30679.4539, 130.6429, 1.4653\n"
     ]
    }
   ],
   "source": [
    "print \"Linear Model- Mean squared error, mean absolute error, root mean squared log error: %2.4f, %2.4f, %2.4f\" %(mse,mae,rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model- Mean squared error, mean absolute error, root mean squared log error: 11560.7978, 71.0969, 0.6259\n"
     ]
    }
   ],
   "source": [
    "mse_dt = true_vs_predicted_dt.map(lambda(t,p):squared_error(t,p)).mean()\n",
    "mae_dt = true_vs_predicted_dt.map(lambda(t,p):abs_error(t,p)).mean()\n",
    "rmsle_dt = np.sqrt(true_vs_predicted_dt.map(lambda (t,p):squared_log_error(t,p)).mean())\n",
    "\n",
    "print \"Decision Tree model- Mean squared error, mean absolute error, root mean squared log error: %2.4f, %2.4f, %2.4f\" %(mse_dt,mae_dt,rmsle_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
